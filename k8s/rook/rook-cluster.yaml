apiVersion: rook.io/v1alpha1
kind: Cluster
metadata:
  name: rook
  namespace: rook
spec:
  # the storage backend (only ceph is currently implemented)
  backend: ceph
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # toggle to use hostNetwork
  hostNetwork: false
  # set the amount of monitors to be started
  monCount: 3
# To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
# The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage' and
# tolerate taints with a key of 'storage-node'.
  placement:
    all:
      nodeAffinity:
      podAffinity:
      podAntiAffinity:
      tolerations:
      - key: storage-node
        operator: Exists
#    mgr:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: storage
#              operator: In
#              values:
#              - ceph
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: storage
              operator: In
              values:
              - ceph
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
    mgr:
      limits:
        cpu: "500m"
        memory: "1024Mi"
      requests:
        cpu: "500m"
        memory: "1024Mi"
    mon:
      limits:
        cpu: "500m"
        memory: "1024Mi"
      requests:
        cpu: "500m"
        memory: "1024Mi"
    osd:
      limits:
        cpu: "500m"
        memory: "1024Mi"
      requests:
        cpu: "500m"
        memory: "300Mi"
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    metadataDevice:
    location:
    storeConfig:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      storeType: bluestore # filestore  # bluestore
      databaseSizeMB: 1024 # this value can be removed for environments with normal sized disks (100 GB or larger)
      journalSizeMB: 1024  # this value can be removed for environments with normal sized disks (20 GB or larger)
    nodes:
#    - name: "k8s-node-1"
#      devices:
#      - name: "vdb1"
    - name: "k8s-m3"
      devices:
      - name: "sdb"
    - name: "k8s-n2"
      devices:
      - name: "sdb"
    - name: "ceph-t1"
      devices:
      - name: "vdc"
    - name: "ceph-t2"
      devices:
      - name: "vdc"
    - name: "ceph-t3"
      devices:
      - name: "vdc"
#      directories: # specific directories to use for storage can be specified for each node
#      - path: "/data"
#    - name: "172.17.4.201"
#      devices: # specific devices to use for storage can be specified for each node
#      - name: "sdb"
#      - name: "sdc"
#      storeConfig: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: bluestore
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."
